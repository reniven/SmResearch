{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import collections\n",
    "import pandas as pd\n",
    "import tldextract\n",
    "import requests\n",
    "from urllib.parse import urlparse, urlsplit, urlunsplit\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresExtraction(features_file):\n",
    "    convertedArray = json.load(open(features_file, 'r'))\n",
    "    features = []\n",
    "    for i, j in enumerate(convertedArray):\n",
    "        if 'retweeted_status'in j and 'extended_tweet' in j['retweeted_status']:\n",
    "            features.append(j['retweeted_status']['extended_tweet']['full_text'])\n",
    "        elif 'retweeted_status' not in j and 'extended_tweet' in j:\n",
    "            features.append(j['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            features.append(j['text'])\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelExtraction(label_file):\n",
    "#     csv_file = open(file)\n",
    "#     csv_reader = csv.reader(csv_file, delimiter = ' ')\n",
    "#     labels = []\n",
    "#     for row in csv_reader:\n",
    "#         labels.append(row[0])\n",
    "    labels = np.genfromtxt(label_file,dtype=None)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction(array, key):\n",
    "    reducedArray = []\n",
    "    for i, j in enumerate(array):\n",
    "        if len(reducedArray) == 0:\n",
    "            reducedArray.append(j)\n",
    "        else:\n",
    "            count = 0\n",
    "            for k in range(len(reducedArray)):\n",
    "                if a[k] != b[i]:\n",
    "                    count = count + 1\n",
    "            \n",
    "                if count == len(a):\n",
    "                    a.append(b[i])\n",
    "    \n",
    "    for i, j in enumerate(convertedArray):\n",
    "        if 'retweeted_status'in j and 'extended_tweet' in j['retweeted_status']:\n",
    "            features.append(j['retweeted_status']['extended_tweet']['full_text'])\n",
    "        elif 'retweeted_status' not in j and 'extended_tweet' in j:\n",
    "            features.append(j['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            features.append(j['text'])\n",
    "    return reducedArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCredentials():\n",
    "    app_key = \"jGVy1ilV55A7P24eJHuousM1b\"\n",
    "    app_secret = \"wrPOxGirrXEUQsU7Pk0jqaqt7nLYq10TmR0E6QdZPSlvvrktE8\"\n",
    "    oauth_token = \"1036998187880460289-K5YKVDBIlK5e1bsgsxzvmGk2z2BzTO\"\n",
    "    oauth_token_secret = \"ogxeyO9mgweKrFEOZhn638EvTVvZjn06ynVwviShsVOzZ\"\n",
    "    \n",
    "    return app_key, app_secret, oauth_token, oauth_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresExtraction2(features_file):\n",
    "    convertedArray = json.load(open(features_file, 'r'))\n",
    "    final = []\n",
    "    textFeatures = []\n",
    "    \n",
    "    for i, j in enumerate(convertedArray):\n",
    "        tempDict = {}\n",
    "        tempDict['id'] = j['id']\n",
    "        tempDict['date'] = j['created_at']\n",
    "        tempDict['username'] = j['user']['screen_name']\n",
    "        tempDict['hashtags'] = j['entities']['hashtags']\n",
    "        tempDict['user_mentions'] = j['entities']['user_mentions']\n",
    "        tempDict['favorites'] = j['favorite_count']\n",
    "        \n",
    "        if 'retweeted_status'in j and 'extended_tweet' in j['retweeted_status']:\n",
    "            textFeatures.append(j['retweeted_status']['extended_tweet']['full_text'])\n",
    "        elif 'retweeted_status' not in j and 'extended_tweet' in j:\n",
    "            textFeatures.append(j['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            textFeatures.append(j['text'])\n",
    "        \n",
    "        final.append(tempDict)\n",
    "    \n",
    "    return final, textFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authorization(key):\n",
    "    if key == 'e' or 'E':\n",
    "        app_key = \"jGVy1ilV55A7P24eJHuousM1b\"\n",
    "        app_secret = \"wrPOxGirrXEUQsU7Pk0jqaqt7nLYq10TmR0E6QdZPSlvvrktE8\"\n",
    "        oauth_token = \"1036998187880460289-K5YKVDBIlK5e1bsgsxzvmGk2z2BzTO\"\n",
    "        oauth_token_secret = \"ogxeyO9mgweKrFEOZhn638EvTVvZjn06ynVwviShsVOzZ\"\n",
    "        return app_key, app_secret, oauth_token, oauth_token_secret\n",
    "    elif key == 'v' or 'V':\n",
    "        app_key = \"d\"\n",
    "    else:\n",
    "        print(\"Wrong identification key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cred_Processing(tweetsFile):\n",
    "    idRegEx = re.compile(r\".*ID=\")\n",
    "    endElRegEX = re.compile(r\"'.*\")\n",
    "    output = open(\"outputTweets.json\", \"w\")\n",
    "    \n",
    "    with open(tweetsFile, \"r\") as f:\n",
    "        header = f.readline()\n",
    "        \n",
    "        for line in f:\n",
    "            topicData = line.split(\"\\t\")\n",
    "            \n",
    "            topicKey = topicData[0]\n",
    "            topicTerms = topicData[1]\n",
    "            topicTweetCount = topicData[2]\n",
    "            tweetIdList = topicData[3]\n",
    "            \n",
    "            print(topicKey)\n",
    "            \n",
    "            realTweetsIds = []\n",
    "            \n",
    "            idElements = tweetIdList.split(\"),\")\n",
    "            for element in idElements:\n",
    "                elArr = element.split(\",\")\n",
    "                idEl = list(filter(lambda x: \"ID\" in x, elArr))[0]\n",
    "                idEl = idRegEx.sub(\"\", idEl)\n",
    "                idEl = endElRegEX.sub(\"\", idEl)\n",
    "                \n",
    "                realTweetsIds.append(int(idEl))\n",
    "                \n",
    "            realTweetsIds = list(set(realTweetsIds))\n",
    "            \n",
    "            topicMap = {\n",
    "                \"key\" : topicKey,\n",
    "                \"terms\" : topicTerms.split(\",\"),\n",
    "                \"count\" : topicTweetCount, \n",
    "                \"tweets\" : realTweetsIds\n",
    "            }\n",
    "            \n",
    "            json.dump(topicMap, output, sort_keys=True)\n",
    "            output.write(\"\\n\")\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_processing(ratingFile, tweetsFile):\n",
    "    tweetsMap = {}\n",
    "    \n",
    "    with open(tweetsFile, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                tweetData = json.loads(line)\n",
    "                tweetsMap[tweetData[\"key\"]] = tweetData\n",
    "            except:\n",
    "                print(\"Error\")\n",
    "            \n",
    "    output = open(r\"/media/eric/TOSHIBA EXT/University/Research/Data/CREDBANK/finalOutput.json\", \"w\")\n",
    "    \n",
    "    with open(ratingFile, \"r\") as f:\n",
    "        header = f.readline()\n",
    "        \n",
    "        for line in f:\n",
    "            topicData = line.split(\"\\t\")\n",
    "            topicKey = topicData[0]\n",
    "            topicTerms = topicData[1]\n",
    "            ratings = topicData[2]\n",
    "            reasons = topicData[3]\n",
    "            \n",
    "            try:\n",
    "                ratings = list(map(lambda x: int(x.strip().replace(\"'\", \"\")), ratings.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")))\n",
    "                ratings = np.array(ratings)\n",
    "                tweetsMap[topicKey][\"mean\"] = np.mean(ratings)\n",
    "                print(\"Inserting ratings for {}\".format(topicKey))\n",
    "                tweetsMap[topicKey][\"ratings\"] = ratings.tolist()\n",
    "                print(\"Success\")\n",
    "                print(\"Inserting mean for {}\".format(topicKey))\n",
    "                tweetsMap[topicKey][\"mean\"] = np.mean(ratings)\n",
    "                print(\"Success\")\n",
    "                topicMap = tweetsMap[topicKey]\n",
    "                print(topicMap[\"key\"], topicMap[\"mean\"])\n",
    "                json.dump(topicMap, output, sort_keys=True)\n",
    "                output.write(\"\\n\")\n",
    "            except:\n",
    "                print(\"{} is missing.\".format(topicKey))\n",
    "            \n",
    "            \n",
    "            \n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryConversion(outputFile):\n",
    "    labels = {}\n",
    "    with open(outputFile, 'r') as f:\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            if(tweet['mean'] >= 1.9):\n",
    "                labels[tweet['key']] = 1\n",
    "            elif(tweet['mean'] <= 1.467):\n",
    "                labels[tweet['key']] = 0\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Retweet and date\n",
    "def singleMessageFeatureExtraction(tweet_json):\n",
    "    #print(tweet_json)\n",
    "    retweets = 0\n",
    "    try:\n",
    "        retweets = tweet_json[\"tweet\"][\"retweet_status\"]\n",
    "        retweets = 1\n",
    "    except:\n",
    "        retweets = 0\n",
    "    date = tweet_json[\"tweet\"][\"created_at\"].split(\" \")[0]\n",
    "    dates = []\n",
    "    if date == \"Mon\":\n",
    "        dates = [1,0,0,0,0,0,0]\n",
    "    elif date == \"Tue\":\n",
    "        dates = [0,1,0,0,0,0,0]\n",
    "    elif date == \"Wed\":\n",
    "        dates = [0,0,1,0,0,0,0]\n",
    "    elif date == \"Thu\":\n",
    "        dates = [0,0,0,1,0,0,0]\n",
    "    elif date == \"Fri\":\n",
    "        dates = [0,0,0,0,1,0,0]\n",
    "    elif date == \"Sat\":\n",
    "        dates = [0,0,0,0,0,1,0]\n",
    "    elif date == \"Sun\":\n",
    "        dates = [0,0,0,0,0,0,1]\n",
    "    return retweets, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unshorten_url(url):\n",
    "    return requests.head(url, allow_redirects=True).url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messageFeatureExtraction(tweets, tweet_json, threshold):\n",
    "    tweetsFeatures = None\n",
    "    np.array(tweetsFeatures)\n",
    "    count = 0\n",
    "    for topic in topics:\n",
    "        print(\"Starting feature extraction for topic {} of {}\".format(count, len(topics)))\n",
    "        indiTweetFeature = []\n",
    "        \n",
    "        #Number of tweets\n",
    "        indiTweetFeature.append(len(topic))\n",
    "        \n",
    "\n",
    "        for tweet in topic:\n",
    "\n",
    "            #Length of characters 1\n",
    "            print(\"Adding Length of charactrs\")\n",
    "            indiTweetFeature.append(len(tweet))\n",
    "        \n",
    "            print(\"Adding words\")\n",
    "            #Length of words 2\n",
    "            indiTweetFeature.append(len(tweet.split(\" \")))\n",
    "        \n",
    "            print(\"Adding question mark\")\n",
    "            #Contains question mark 3\n",
    "            results = collections.Counter(tweet)\n",
    "            if(results['?'] > 1):\n",
    "                indiTweetFeature.append(1)\n",
    "            else:\n",
    "                indiTweetFeature.append(0)\n",
    "\n",
    "            print('Adding exclamation mark')\n",
    "            #Contains exclamation 4\n",
    "            if(results['!'] > 1):\n",
    "                indiTweetFeature.append(1)\n",
    "            else:\n",
    "                indiTweetFeature.append(0)\n",
    "\n",
    "            print(\"Adding smiles\")\n",
    "            #Contains emoticon smile 5\n",
    "            emojis = \"ðŸ˜Š\"\n",
    "            indiTweetFeature.append(tweet.count(emojis))\n",
    "\n",
    "            print(\"!?\")\n",
    "            #Contains multiple question or exclamation marks 6\n",
    "            if results['?'] and results['!'] > 1:\n",
    "                indiTweetFeature.append(1)\n",
    "            else:\n",
    "                indiTweetFeature.append(0)\n",
    "\n",
    "            print(\"Adding first person pronouns\")\n",
    "            #Contains pronoun first, sceond, third 7\n",
    "            first_pronouns = [\"i\", \"we\"]\n",
    "            tweetSplit = tweet.split(\" \")\n",
    "            for x in range(len(tweetSplit)):\n",
    "                if(tweetSplit[x].lower() in first_pronouns):\n",
    "                    indiTweetFeature.append(1)\n",
    "                    break;\n",
    "                if(x == len(tweetSplit)-1):\n",
    "                    indiTweetFeature.append(0)\n",
    "\n",
    "            print(\"Adding second person pronouns\")\n",
    "            second_pronouns = [\"you\"]\n",
    "            tweetSplit = tweet.split(\" \")\n",
    "            for x in range(len(tweetSplit)):\n",
    "                if(tweetSplit[x].lower() in second_pronouns):\n",
    "                    indiTweetFeature.append(1)\n",
    "                    break;\n",
    "                if(x == len(tweetSplit)-1):\n",
    "                    indiTweetFeature.append(0)\n",
    "\n",
    "            print(\"Adding third person pronouns\")\n",
    "            third_pronouns = [\"he\", \"him,\" \"she\", \"her\", \"they\", \"them\", \"it\"]\n",
    "\n",
    "            tweetSplit = tweet.split(\" \")\n",
    "            for x in range(len(tweetSplit)):\n",
    "                if(tweetSplit[x].lower() in third_pronouns):\n",
    "                    indiTweetFeature.append(1)\n",
    "                    break;\n",
    "                if(x == len(tweetSplit)-1):\n",
    "                    indiTweetFeature.append(0)\n",
    "\n",
    "            print(\"Adding uppercase\")\n",
    "            #Count uppercase letters 8\n",
    "            upperLetterCount = 0\n",
    "            for letter in tweet:\n",
    "                if(letter.isupper()):\n",
    "                    upperLetterCount += 1\n",
    "            indiTweetFeature.append(upperLetterCount)\n",
    "\n",
    "            print(\"Adding url\")\n",
    "            #Number of URLs 9\n",
    "            url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+] |[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet) \n",
    "            indiTweetFeature.append(len(url))\n",
    "        \n",
    "            #Contains popular domain top 100,1000,10000\n",
    "            print(\"Adding popular domain\")\n",
    "            file = pd.read_csv('/media/eric/TOSHIBA EXT/University/Research/Data/top-1m.csv')\n",
    "            try:\n",
    "                urls = tweet_json[count]['tweet']['entities']['urls']\n",
    "            except:\n",
    "                print(tweet_json[count]['tweet'])\n",
    "            top100 = 0\n",
    "            top1000 = 0\n",
    "            top10000 = 0\n",
    "\n",
    "            for url in urls:\n",
    "                urlParsed = None\n",
    "                rank = None\n",
    "                try:\n",
    "                    split_url = urlsplit(url['expanded_url'])   \n",
    "                    shortUrl = ''\n",
    "                    if(split_url.netloc[:4] == 'www.'):\n",
    "                        shortUrl = split_url.netloc[4:]\n",
    "                    else:\n",
    "                        shortUrl = split_url.netloc\n",
    "                    print(shortUrl)\n",
    "                    print(file.loc[file['site'] == shortUrl]['rank'])\n",
    "                    rank = int(file.loc[file['site'] == split_url.netloc]['rank'])\n",
    "                except Exception as e:\n",
    "                    rank = 10001\n",
    "                if rank <= 100:\n",
    "                    top100 = 1\n",
    "                if rank <= 1000:\n",
    "                    top1000 = 1\n",
    "                if rank <= 10000:\n",
    "                    top10000 = 1\n",
    "        \n",
    "            indiTweetFeature.append(top100)\n",
    "            indiTweetFeature.append(top1000)\n",
    "            indiTweetFeature.append(top10000)\n",
    "                \n",
    "            print(\"Adding mention\")\n",
    "            #Contains user mention 10\n",
    "            result = re.findall(\"(^|[^@\\w])@(\\w{1,15})\", tweet)\n",
    "            if(len(result) >= threshold):\n",
    "                indiTweetFeature.append(1)\n",
    "            else:\n",
    "                indiTweetFeature.append(0)\n",
    "    \n",
    "            # Length of hashtags 11\n",
    "            print(\"Adding hashtags\")\n",
    "            hashtags = [i  for i in tweet.split() if i.startswith(\"#\") ]\n",
    "            indiTweetFeature.append(len(hashtags))\n",
    "     \n",
    "            #Contains stock symbol 12\n",
    "            print(\"Addings stocks\")\n",
    "            file = open('/media/eric/TOSHIBA EXT/University/Research/Data/nasdaqlisted.txt', 'r')\n",
    "            filecontents = file.readlines()\n",
    "            stock_symbols = []\n",
    "            for line in filecontents:\n",
    "                splitline = line.split(\"|\")\n",
    "                stock_symbols.append(splitline[0])\n",
    "            \n",
    "            del stock_symbols[0]\n",
    "        \n",
    "            stockCount = 0\n",
    "            for word in tweetSplit:\n",
    "                if(word in stock_symbols):\n",
    "                    stockCount += 1\n",
    "        \n",
    "            if(stockCount > 0):\n",
    "                indiTweetFeature.append(1)\n",
    "            else:\n",
    "                indiTweetFeature.append(0)\n",
    "                \n",
    "            retweets, date = singleMessageFeatureExtraction(tweet_json[count])\n",
    "            print(\"Adding retweets\")\n",
    "            indiTweetFeature.append(retweets)\n",
    "            \n",
    "            print(\"Adding date\")\n",
    "            indiTweetFeature = indiTweetFeature + date\n",
    "        \n",
    "            print(\"Adding sentiment\")\n",
    "            #Sentiment 13, 14, 15\n",
    "            positiveWords = 0\n",
    "            negativeWords= 0\n",
    "    \n",
    "    \n",
    "            tweetSentiment = TextBlob(tweet)\n",
    "            sentimentScore = tweetSentiment.sentiment[0]\n",
    "    \n",
    "            for word in tweetSentiment:\n",
    "                word = TextBlob(word)\n",
    "                if(word.sentiment.polarity) > 0:\n",
    "                    positiveWords = positiveWords + 1\n",
    "                elif(word.sentiment.polarity) < 0:\n",
    "                    negativeWords = negativeWords + 1\n",
    "            indiTweetFeature.append(positiveWords)\n",
    "            indiTweetFeature.append(negativeWords)\n",
    "            indiTweetFeature.append(sentimentScore)\n",
    "        \n",
    "        \n",
    "            indiTweetFeature = [indiTweetFeature]\n",
    "            if(count == 0):\n",
    "                tweetsFeatures = indiTweetFeature\n",
    "                print(\"Length of array: {}\".format(len(tweetsFeatures[count])))\n",
    "            else:\n",
    "                tweetsFeatures = np.append(tweetsFeatures, indiTweetFeature, axis = 0)\n",
    "                print(tweetsFeatures.shape)\n",
    "        \n",
    "            print(\"Finished feature extraction for tweet {} of {}\".format(count, len(tweets)))\n",
    "            count += 1\n",
    "        \n",
    "    return topicFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topicFeatureExtraction(topics):\n",
    "    \n",
    "    for topic in topics:\n",
    "        \n",
    "        urls = []\n",
    "        \n",
    "        for tweet in topic:\n",
    "        print(\"Adding popular domain\")\n",
    "        file = pd.read_csv('/media/eric/TOSHIBA EXT/University/Research/Data/top-1m.csv')\n",
    "        #Fraction Popular Domain Top 100, 1000, 10,000\n",
    "        \n",
    "        try:\n",
    "            urls = tweet_json[count]['tweet']['entities']['urls']\n",
    "        except:\n",
    "            print(tweet_json[count]['tweet'])\n",
    "        top100 = 0\n",
    "        top1000 = 0\n",
    "        top10000 = 0\n",
    "\n",
    "        for url in urls:\n",
    "            urlParsed = None\n",
    "            rank = None\n",
    "            try:\n",
    "                urls.append(url['expanded_url'])\n",
    "                split_url = urlsplit(url['expanded_url'])   \n",
    "                shortUrl = ''\n",
    "                if(split_url.netloc[:4] == 'www.'):\n",
    "                    shortUrl = split_url.netloc[4:]\n",
    "                else:\n",
    "                    shortUrl = split_url.netloc\n",
    "                print(shortUrl)\n",
    "                print(file.loc[file['site'] == shortUrl]['rank'])\n",
    "                rank = int(file.loc[file['site'] == split_url.netloc]['rank'])\n",
    "            except Exception as e:\n",
    "                rank = 10001\n",
    "            if rank <= 100:\n",
    "                top100 = 1\n",
    "            if rank <= 1000:\n",
    "                top1000 = 1\n",
    "            if rank <= 10000:\n",
    "                top10000 = 1\n",
    "    \n",
    "        #Count Distinct Seemingly Shortened URLs\n",
    "    \n",
    "\n",
    "    \n",
    "        #Count Distinct Users Mentioned\n",
    "    \n",
    "        #Share most Frequent Users Mentioned\n",
    "    \n",
    "        #Count Distinct Authors\n",
    "    \n",
    "        #Share most Frequent Authors\n",
    "    \n",
    "        #Author Average Registration AGe\n",
    "    \n",
    "        #Author Average Statuses Count\n",
    "    \n",
    "        #Author Average Count Followers\n",
    "    \n",
    "        #Author Average Count Friends\n",
    "    \n",
    "        #Author Fraction is Verified\n",
    "    \n",
    "        #Author Fraction has Description\n",
    "    \n",
    "        #Author Fraction has URL\n",
    "        \n",
    "    #Count Distinct Expanded URLs\n",
    "    \n",
    "    #Share most Frequent Expanded URL\n",
    "    \n",
    "    #Count Distinct Hashtags\n",
    "    \n",
    "    #Share most Frequent Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userFeatureExtraction():\n",
    "    \n",
    "    #Registration Age\n",
    "    \n",
    "    #Number of followers\n",
    "    \n",
    "    #Statuses Count (No. of Tweets)\n",
    "    \n",
    "    #Count Friends\n",
    "    \n",
    "    #Is Verified\n",
    "    \n",
    "    #Has Bio\n",
    "    \n",
    "    #Has URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagationFeatureExtraction():\n",
    "    \n",
    "    #Propogation Initial Tweets\n",
    "    \n",
    "    #Propagation Max Subtree\n",
    "    \n",
    "    #Propagation Average Degree\n",
    "    \n",
    "    #Propagation Max Average Depth\n",
    "    \n",
    "    #Propagation Max Level"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
