{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresExtraction(features_file):\n",
    "    convertedArray = json.load(open(features_file, 'r'))\n",
    "    features = []\n",
    "    for i, j in enumerate(convertedArray):\n",
    "        if 'retweeted_status'in j and 'extended_tweet' in j['retweeted_status']:\n",
    "            features.append(j['retweeted_status']['extended_tweet']['full_text'])\n",
    "        elif 'retweeted_status' not in j and 'extended_tweet' in j:\n",
    "            features.append(j['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            features.append(j['text'])\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelExtraction(label_file):\n",
    "#     csv_file = open(file)\n",
    "#     csv_reader = csv.reader(csv_file, delimiter = ' ')\n",
    "#     labels = []\n",
    "#     for row in csv_reader:\n",
    "#         labels.append(row[0])\n",
    "    labels = np.genfromtxt(label_file,dtype=None)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction(array, key):\n",
    "    reducedArray = []\n",
    "    for i, j in enumerate(array):\n",
    "        if len(reducedArray) == 0:\n",
    "            reducedArray.append(j)\n",
    "        else:\n",
    "            count = 0\n",
    "            for k in range(len(reducedArray)):\n",
    "                if a[k] != b[i]:\n",
    "                    count = count + 1\n",
    "            \n",
    "                if count == len(a):\n",
    "                    a.append(b[i])\n",
    "    \n",
    "    for i, j in enumerate(convertedArray):\n",
    "        if 'retweeted_status'in j and 'extended_tweet' in j['retweeted_status']:\n",
    "            features.append(j['retweeted_status']['extended_tweet']['full_text'])\n",
    "        elif 'retweeted_status' not in j and 'extended_tweet' in j:\n",
    "            features.append(j['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            features.append(j['text'])\n",
    "    return reducedArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCredentials():\n",
    "    app_key = \"jGVy1ilV55A7P24eJHuousM1b\"\n",
    "    app_secret = \"wrPOxGirrXEUQsU7Pk0jqaqt7nLYq10TmR0E6QdZPSlvvrktE8\"\n",
    "    oauth_token = \"1036998187880460289-K5YKVDBIlK5e1bsgsxzvmGk2z2BzTO\"\n",
    "    oauth_token_secret = \"ogxeyO9mgweKrFEOZhn638EvTVvZjn06ynVwviShsVOzZ\"\n",
    "    \n",
    "    return app_key, app_secret, oauth_token, oauth_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresExtraction2(features_file):\n",
    "    convertedArray = json.load(open(features_file, 'r'))\n",
    "    final = []\n",
    "    textFeatures = []\n",
    "    \n",
    "    for i, j in enumerate(convertedArray):\n",
    "        tempDict = {}\n",
    "        tempDict['id'] = j['id']\n",
    "        tempDict['date'] = j['created_at']\n",
    "        tempDict['username'] = j['user']['screen_name']\n",
    "        tempDict['hashtags'] = j['entities']['hashtags']\n",
    "        tempDict['user_mentions'] = j['entities']['user_mentions']\n",
    "        tempDict['favorites'] = j['favorite_count']\n",
    "        \n",
    "        if 'retweeted_status'in j and 'extended_tweet' in j['retweeted_status']:\n",
    "            textFeatures.append(j['retweeted_status']['extended_tweet']['full_text'])\n",
    "        elif 'retweeted_status' not in j and 'extended_tweet' in j:\n",
    "            textFeatures.append(j['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            textFeatures.append(j['text'])\n",
    "        \n",
    "        final.append(tempDict)\n",
    "    \n",
    "    return final, textFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authorization(key):\n",
    "    if key == 'e' or 'E':\n",
    "        app_key = \"jGVy1ilV55A7P24eJHuousM1b\"\n",
    "        app_secret = \"wrPOxGirrXEUQsU7Pk0jqaqt7nLYq10TmR0E6QdZPSlvvrktE8\"\n",
    "        oauth_token = \"1036998187880460289-K5YKVDBIlK5e1bsgsxzvmGk2z2BzTO\"\n",
    "        oauth_token_secret = \"ogxeyO9mgweKrFEOZhn638EvTVvZjn06ynVwviShsVOzZ\"\n",
    "        return app_key, app_secret, oauth_token, oauth_token_secret\n",
    "    elif key == 'v' or 'V':\n",
    "        app_key = \"d\"\n",
    "    else:\n",
    "        print(\"Wrong identification key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cred_Processing(tweetsFile):\n",
    "    idRegEx = re.compile(r\".*ID=\")\n",
    "    endElRegEX = re.compile(r\"'.*\")\n",
    "    output = open(\"outputTweets.json\", \"w\")\n",
    "    \n",
    "    with open(tweetsFile, \"r\") as f:\n",
    "        header = f.readline()\n",
    "        \n",
    "        for line in f:\n",
    "            topicData = line.split(\"\\t\")\n",
    "            \n",
    "            topicKey = topicData[0]\n",
    "            topicTerms = topicData[1]\n",
    "            topicTweetCount = topicData[2]\n",
    "            tweetIdList = topicData[3]\n",
    "            \n",
    "            print(topicKey)\n",
    "            \n",
    "            realTweetsIds = []\n",
    "            \n",
    "            idElements = tweetIdList.split(\"),\")\n",
    "            for element in idElements:\n",
    "                elArr = element.split(\",\")\n",
    "                idEl = list(filter(lambda x: \"ID\" in x, elArr))[0]\n",
    "                idEl = idRegEx.sub(\"\", idEl)\n",
    "                idEl = endElRegEX.sub(\"\", idEl)\n",
    "                \n",
    "                realTweetsIds.append(int(idEl))\n",
    "                \n",
    "            realTweetsIds = list(set(realTweetsIds))\n",
    "            \n",
    "            topicMap = {\n",
    "                \"key\" : topicKey,\n",
    "                \"terms\" : topicTerms.split(\",\"),\n",
    "                \"count\" : topicTweetCount, \n",
    "                \"tweets\" : realTweetsIds\n",
    "            }\n",
    "            \n",
    "            json.dump(topicMap, output, sort_keys=True)\n",
    "            output.write(\"\\n\")\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_processing(ratingFile, tweetsFile):\n",
    "    tweetsMap = {}\n",
    "    \n",
    "    with open(tweetsFile, \"r\") as f:\n",
    "        for line in f:\n",
    "            tweetData = json.loads(line)\n",
    "            tweetsMap[tweetData[\"key\"]] = tweetData\n",
    "            \n",
    "    output = open(r\"C:\\Users\\eric4\\Desktop\\University\\Research\\Data\\CREDBANK\\finalOutput.json\", \"w\")\n",
    "    \n",
    "    with open(ratingFile, \"r\") as f:\n",
    "        header = f.readline()\n",
    "        \n",
    "        for line in f:\n",
    "            topicData = line.split(\"\\t\")\n",
    "            topicKey = topicData[0]\n",
    "            topicTerms = topicData[1]\n",
    "            ratings = topicData[2]\n",
    "            reasons = topicData[3]\n",
    "            \n",
    "            ratings = list(map(lambda x: int(x.strip().replace(\"'\", \"\")), ratings.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")))\n",
    "            ratings = np.array(ratings)\n",
    "            tweetsMap[topicKey][\"mean\"] = np.mean(ratings)\n",
    "            try:\n",
    "                print(\"Inserting ratings for {}\".format(topicKey))\n",
    "                tweetsMap[topicKey][\"ratings\"] = ratings.tolist()\n",
    "                print(\"Success\")\n",
    "                print(\"Inserting mean for {}\".format(topicKey))\n",
    "                tweetsMap[topicKey][\"mean\"] = np.mean(ratings)\n",
    "                print(\"Success\")\n",
    "                topicMap = tweetsMap[topicKey]\n",
    "                print(topicMap[\"key\"], topicMap[\"mean\"])\n",
    "                json.dump(topicMap, output, sort_keys=True)\n",
    "                output.write(\"\\n\")\n",
    "            except:\n",
    "                print(\"{} is missing.\".format(topicKey))\n",
    "            \n",
    "            \n",
    "            \n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryConversion(outputFile):\n",
    "    labels = {}\n",
    "    with open(outputFile, 'r') as f:\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            if(tweet['mean'] >= 1.9):\n",
    "                labels[tweet['key']] = 1\n",
    "            elif(tweet['mean'] <= 1.467):\n",
    "                labels[tweet['key']] = 0\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messageFeatureExtraction(tweets, threshold):\n",
    "    tweetsFeatures = []\n",
    "    np.array(tweetsFeatures)\n",
    "    count = 1\n",
    "    for tweet in tweets:\n",
    "        print(\"Starting feature extraction for tweet {} of {}\".format(count, len(tweets)))\n",
    "        indiTweetFeature = []\n",
    "        #Length of characters 1\n",
    "        print(\"Adding Length of charactrs\")\n",
    "        indiTweetFeature.append(len(tweet))\n",
    "        #print(indiTweetFeature)\n",
    "        \n",
    "        print(\"Adding words\")\n",
    "        #Length of words 2\n",
    "        indiTweetFeature.append(len(tweet.split(\" \")))\n",
    "        #print(indiTweetFeature)\n",
    "        \n",
    "        print(\"Adding question mark\")\n",
    "        #Contains question mark 3\n",
    "        results = collections.Counter(tweet)\n",
    "        if(results['?'] > 1):\n",
    "            indiTweetFeature.append(1)\n",
    "        else:\n",
    "            indiTweetFeature.append(0)\n",
    "        #print(indiTweetFeature)\n",
    "\n",
    "        print('Adding question mark')\n",
    "        #Contains exclamation 4\n",
    "        if(results['!'] > 1):\n",
    "            indiTweetFeature.append(1)\n",
    "        else:\n",
    "            indiTweetFeature.append(0)\n",
    "        #print(indiTweetFeature)\n",
    "\n",
    "        print(\"Adding smiles\")\n",
    "        #Contains emoticon smile 5\n",
    "        emojis = \"😊\"\n",
    "        indiTweetFeature.append(tweet.count(emojis))\n",
    "        #print(indiTweetFeature)\n",
    "\n",
    "        print(\"!?\")\n",
    "        #Contains multiple question or exclamation marks 6\n",
    "        if results['?'] and results['!'] > 1:\n",
    "            indiTweetFeature.append(1)\n",
    "        else:\n",
    "            indiTweetFeature.append(0)\n",
    "        #print(indiTweetFeature)\n",
    "\n",
    "        print(\"Adding pronouns\")\n",
    "        #Contains pronoun first, sceond, third 7\n",
    "        pronouns = [\"i\", \"he\", \"him,\" \"she\", \"her\", \"they\", \"them\", \"it\", \"you\"]\n",
    "        tweetSplit = tweet.split(\" \")\n",
    "        for x in range(len(tweetSplit)):\n",
    "            if(tweetSplit[x].lower() in pronouns):\n",
    "                indiTweetFeature.append(1)\n",
    "                break;\n",
    "            if(x == len(tweetSplit)-1):\n",
    "                indiTweetFeature.append(0)\n",
    "        #print(indiTweetFeature)    \n",
    "        \n",
    "        print(\"Adding uppercase\")\n",
    "        #Count uppercase letters 8\n",
    "        upperLetterCount = 0\n",
    "        for letter in tweet:\n",
    "            if(letter.isupper()):\n",
    "                upperLetterCount += 1\n",
    "        indiTweetFeature.append(upperLetterCount)\n",
    "        #print(indiTweetFeature)\n",
    "\n",
    "        print(\"Adding url\")\n",
    "        #Number of URLs 9\n",
    "        url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+] |[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet) \n",
    "        indiTweetFeature.append(len(url))\n",
    "        #print(indiTweetFeature)\n",
    "        \n",
    "        print(\"mention\")\n",
    "        #Contains user mention 10\n",
    "        result = re.findall(\"(^|[^@\\w])@(\\w{1,15})\", tweet)\n",
    "        if(len(result) >= threshold):\n",
    "            indiTweetFeature.append(1)\n",
    "        else:\n",
    "            indiTweetFeature.append(0)\n",
    "        #print(indiTweetFeature)\n",
    "\n",
    "#         print(\"hashtags\")\n",
    "#         #Contains hashtags 11\n",
    "#         for x in range(len(tweetSplit)):\n",
    "#             for letter in range(len(tweetSplit[x])):\n",
    "#                 if(tweetSplit[x][letter] == \"#\"):\n",
    "#                     indiTweetFeature.append(1)\n",
    "#                     break\n",
    "#             if(x == len(tweetSplit)-1):\n",
    "#                 indiTweetFeature.append(0)\n",
    "#             else:\n",
    "#                 break\n",
    "#         print(indiTweetFeature)\n",
    "     \n",
    "        #Contains stock symbol\n",
    "#         file = open('../../Data/nasdaqlisted.txt', 'r')\n",
    "#         filecontents = file.readlines()\n",
    "#         stock_symbols = []\n",
    "#         for line in filecontents:\n",
    "#             splitline = line.split(\"|\")\n",
    "#             stock_symbols.append(splitline[0])\n",
    "            \n",
    "#         del stock_symbols[0]\n",
    "        \n",
    "#         for word in tweetSplit:\n",
    "#             if(word in stock_symbols):\n",
    "#                 indiTweetFeature.append(1)\n",
    "#             else:\n",
    "#                 indiTweetFeature.append(0)\n",
    "                \n",
    "        #Reweet\n",
    "        \n",
    "        #Day\n",
    "        \n",
    "        print(\"Adding sentiment\")\n",
    "        #Sentiment 12, 13, 14\n",
    "        positiveWords = 0\n",
    "        negativeWords= 0\n",
    "    \n",
    "    \n",
    "        tweetSentiment = TextBlob(tweet)\n",
    "        sentimentScore = tweetSentiment.sentiment[0]\n",
    "    \n",
    "        for word in tweetSentiment:\n",
    "            word = TextBlob(word)\n",
    "            if(word.sentiment.polarity) > 0:\n",
    "                positiveWords = positiveWords + 1\n",
    "            elif(word.sentiment.polarity) < 0:\n",
    "                negativeWords = negativeWords + 1\n",
    "        indiTweetFeature.append(positiveWords)\n",
    "        indiTweetFeature.append(negativeWords)\n",
    "        indiTweetFeature.append(sentimentScore)\n",
    "        #print(indiTweetFeature)\n",
    "        np.append(tweetsFeatures, np.array(indiTweetFeature), axis = 0)\n",
    "        #print(len(indiTweetFeature))\n",
    "        print(\"Finished feature extraction for tweet {} of {}\".format(count, len(tweets)))\n",
    "        count += 1\n",
    "    return tweetsFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
